@{
    ViewBag.Title = "Step4";
    Layout = null; 
}
@model EvalEngine.UI.Models.Step4Model
@using Svg;
@using System.Text;
@using EvalEngine.UI.Helpers;
@using EvalEngine.Domain.Entities
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
<head>
<meta http-equiv="X-UA-Compatible" content="IE=9; IE=8; IE=7; IE=EDGE" />
<meta http-equiv="Content-type" content="text/html; charset=ISO-8859-1">
<title>@ViewBag.Title</title>
<script src="@Url.Content("~/Content/inc/js/d3.v3.min.js")" type="text/javascript"></script>
<script src="@Url.Content("~/Content/inc/js/dimple.v2.1.6.min.js")" type="text/javascript"></script>
<script src="@Url.Content("~/Scripts/jquery-1.5.1.min.js")" type="text/javascript"></script>
<link rel="stylesheet" href="@Url.Content("~/Content/inc/css/reset-fonts-grids.css")" type="text/css" />
<link rel="stylesheet" href="@Url.Content("~/Content/inc/css/pdf-styles.css")" type="text/css" />
<link rel="stylesheet" href="@Url.Content("~/Content/inc/css/jquery-ui.css")" type="text/css" /> 
<style>
 
body 
{
    margin-left:100px;
    font-family: "Gill Sans", "Gill Sans MT", Calibri, sans-serif;
}
    
div.pagebreak {page-break-after:always;}
div.footerpp {page-break-after:always;}
div.chart-container{page-break-before:auto;}

.dropdown div{
	display:none;
}

.dropdown:hover div{
	display:block;
	position:absolute;
	background-color:#fff;
	border:#D4DADE 1px solid;
	padding:10px;
	margin-top:-3px;
	-webkit-box-shadow: 2px 2px 2px #ddd;	
	-moz-box-shadow: 2px 2px 2px #ddd;
	box-shadow: 2px 2px 2px #ddd;
	width:130px;
}

ul.dropdown-nav{
	margin-left:0;
	padding-left:0;
}

ul.dropdown-nav li{
	list-style-type:none;

}

td.a-right{
	text-align:center;
}

div.coverpage
{
    margin-top:20px;margin-bottom:5px;background:white;margin-left:-200px;
    height:15in;
    width:11in;
    border:1px solid #6C7F8C;
}

div.coverpage p.header
{
	color:#6C7F8C; 
	font-size:50px;
	font-weight:600;
	padding-bottom:15px;
	text-shadow:.6px .5px #efefef;
	text-align:center;
}
div.coverpage p
{
	color:#6C7F8C; 
	font-size:35px;
	font-weight:600;
	padding-bottom:15px;
	text-shadow:.6px .5px #efefef;
	text-transform:none;
	text-align:center;
}

div.coverpage img
{
    padding-top:20px;
    margin-left:auto;
    margin-right:auto;
    display: block;
}

.indent
{
    padding-left:20px;
    width: 1100px;
}

div.bottom
{
    position:absolute;
    vertical-align:bottom;
    top:14.5in;
    width:inherit;
    text-align:center;
    display:none;
}

.ee-logo{
	margin-left:21px;
}

.header-line{
	border-bottom:#C1C1C1 2px solid;
	float:right;
	margin:-29px 0 0 352px;
	position:absolute;
	width:409px; 
	-webkit-box-shadow: 1px 1px 1px #fff;	
	-moz-box-shadow: 1px 1px 1px #fff;	
	box-shadow: 1px 1px 1px #fff;
}

h1
{
    text-transform:uppercase;
    color:rgb(108, 127, 140);
}

.vcenter
{
    height:14in;
    width:11in;
    vertical-align:middle;
    display: table-cell;
}

div.section-header
{
    width:1200px;color:White;background:#1E487D;
    padding:10px 5px 0px 5px;margin-left:-100px;
}

div.section-header h1
{
    margin-left:100px;
    color:#fff;
    font-weight:lighter;
    font-size:33px;
    font-family: "Gill Sans", "Gill Sans MT", Calibri, sans-serif;
}

div.sub-header
{
    width:870px;color:White;background:#95B3D7;
    padding:10px 10px 10px 10px;margin-left:0px;
    margin-bottom:5px;
}

div.sub-header span
{
    
    color:#fff;
    font-weight:lighter;
    font-size:25px;
    font-family: "Gill Sans", "Gill Sans MT", Calibri, sans-serif;
    padding-top:10px;
    line-height:1em;
    width:780px;
}

div.sub-header h2
{
    
    color:#fff;
    font-weight:lighter;
    font-size:25px;
    font-family: "Gill Sans", "Gill Sans MT", Calibri, sans-serif;
    padding-top:10px;
    line-height:1em;

    margin:0px;
    padding:0px;
    text-shadow:none;
}
div.pp_overall svg rect.dimple-control, div.pp svg rect.dimple-control, div.ss svg rect.dimple-control  {
    stroke: #325D87;
    fill: #325D87;
}
div.pp_overall svg rect.dimple-treatment, div.pp svg rect.dimple-treatment, div.ss svg rect.dimple-treatment  {
    stroke: #E8750C;
    fill: #E8750C;
}
sup
{
    font-size: 75%;
    line-height: 0;
    position:relative;
    vertical-align:baseline;
    top: -0.5em;
}
sub
{
    font-size: 75%;
    line-height: 0;
    position:relative;
    vertical-align:baseline;
    bottom: -0.25em;
}

div.supplemental table:nth-of-type(2):not(.table):not(.data):not(.appendix), div.supplemental table:nth-of-type(3):not(.table):not(.data):not(.appendix), div.supplemental table:nth-of-type(4):not(.table):not(.data):not(.appendix) {
    width: 1200px;
    margin-left: -100px;
}

</style>
</head>
<body>
       <!--<div class="section-header"><h1>Table of Contents</h1></div>
        <br /><br /><br />
        <div class="toc">
                <ul style="list-style-type:none;">
                <li><a href="#OverallResults">Overall Results</a></li>
                <li><a href="#SubgroupAnalysis">Subgroup Analysis</a></li>
                <li><a href="#StatisticalMethods">Data and Statistical Methods</a></li>
                <ul style="list-style-type:none;">
                    <li><a href="#sm1">Data</a></li>
                    <li><a href="#sm2">Measured Covariates</a></li>
                    <li><a href="#sm3">Balance Plot</a></li>
                    <li><a href="#sm4">Matching procedure</a></li>
                    <li><a href="#sm5">Outcome Analysis</a></li>
                    <li><a href="#sm6">Statistically significant main effect</a></li>
                    <li><a href="#sm7">Non-significant main effect</a></li>
                    <li><a href="#sm8">Effect estimates</a></li>
                    <li><a href="#sm9">Sensitivity to unmeasured variables</a></li>
                </ul>
                <li><a href="#UserDescriptions">User Descriptions</a></li>
                </ul>
        </div>
        <div class="pagebreak"></div>-->
<div class="section-header">
    <h1>1. Overall Results</h1>
</div><a id="OverallResults"></a>
      <div class="indent">
        <p>This report shows differences between participants in the intervention of interest (@Model.Report.StudyName) and a comparison group of students with similar or identical background 
        characteristics and prior academic achievement. Information about participants and control group students is drawn from the state’s longitudinal data system (SLDS).</p>
        <br />
        <p>The SLDS contained suitable matches for @Model.Report.TreatmentCount program participants@(Model.Report.IsMultiGrade ? " in grades " + Model.Report.GradeList + " at the outset of the the intervention" : ""). Each of these @Model.Report.TreatmentCount participants was matched to one, two, three or four 
        non-participants, with a total of @Model.Report.ControlCount distinct non-participants serving as matched controls.</p>
        <br />
        <p>The impact estimates for @Model.Report.StudyName are presented in the figures and tables that follow. The figures show the combined estimates (comparisons of averages 
        over all matched participants and over all matched comparison values), while the tables show subgroup-specific estimates (if requested).</p>
		<br />
      
		@foreach (EvalEngine.UI.Models.ReportChart chart in Model.Report.ChartCollection)
  {
      if (String.IsNullOrEmpty(chart.ChartData_PerGrade))
      {
          <div class="chart-container">
          @if (chart.Type != "pp")
          {
              <div class="sub-header"><span>OUTCOME: @chart.Title</span></div>
              <p style="color: #800000; font-style: oblique; margin-bottom: 10px;">@chart.OutcomeTitle</p>
              <p style="padding-left: 50px;">@chart.Header</p>
          }

              @Html.Partial("Charting/Bar", chart)

              @Html.Raw(chart.Footer)
              <p style="padding-top: 7px;"><span class="sm-font">Notes: @Html.Raw(@chart.OutcomeNote)</span></p>
              <!--<p style="padding-top:7px;"><span class="sm-font">Notes from the analyst: @chart.Note</span></p>-->

              <br/>
          </div>
      }
      else
      {
          <div class="chart-container">
          @if (chart.Type != "pp")
          {
              <p style="border-bottom: #D4DADE 1px solid; margin-bottom: 10px; padding-bottom: 10px; padding-top: 10px; font-weight: 600;">
                  OUTCOME: @chart.Title</p>
              <p style="color: #800000; font-style: oblique; margin-bottom: 10px;">@chart.OutcomeTitle</p>
          }
          
              @if (chart.Type.ToString() == "ss")
              {
                  @Html.Partial("Charting/EffectSize", chart)
              }
              else if (chart.Type != "stacked")
              {
                  @Html.Partial("Charting/OverallProficiency", chart)
              }


              <div style="border: #D4DADE 0px solid; margin-top: 20px; margin-bottom: 5px; padding-left: 0px;">
                  <div id="chart@(chart.Rank.ToString())@(chart.Type.ToString())" class="@(chart.Type.ToString())"></div>
                  @if (chart.Type != "stacked")
                  {
                      @Html.Partial("Charting/MultiGrade", chart)
                  }
                  else
                  {
                      @Html.Partial("Charting/StackedBar", chart)
                  }

              </div>
              <p>@chart.Header</p>
              @Html.Raw(chart.Footer)
              <p style="padding-top: 7px;"><span class="sm-font">@Html.Raw(@chart.OutcomeNote)</span></p>
              <!--<p style="padding-top:7px;"><span class="sm-font">Notes from the analyst: @chart.Note</span></p>-->

              <br/>
          </div>
      }
  }
  
       </div>
<div class="pagebreak"></div>
		<div class="section-header"><h1>2. Subgroup Analysis</h1></div><a id="SubgroupAnalysis" ></a>
		@foreach (EvalEngine.UI.Models.ReportTable table in Model.Report.TableCollection)
  {
      if (!table.HTML.Contains("There was an error parsing the YAML for this subgroup analysis"))
      {       
		 <div class="sub-header"><span>OUTCOME: @table.Title</span></div>
		 <p style="color: #800000; font-style: oblique; margin-bottom: 10px;">@table.Subtitle</p>
      
         @Html.Raw(table.HTML)
		 
		 @Html.Raw(table.Footer)
      
      <p style="padding-top:7px;"><span class="sm-font">Notes: @table.SubgroupNote</span></p>
		  <br /><br />
      }
  }
  @if ((Model.Report.Subgroups == "" || Model.Report.Subgroups == null))
  {
    <br /><p>No subgroup variable was selected for this report.</p>
  }
          <div class="pagebreak"></div>
        <div class="section-header"><h1>3. Data and Statistical Methods</h1></div><a id="StatisticalMethods"></a>
	<!-- NB: BEGIN Data and Statistical Methods SECTION TEXT-->
      <div class="indent">
        <p>This part of the report describes the data used to evaluate the participant group (@Model.Report.StudyName), the analytic method used to assess differences in outcomes for the participant 
        group compared with the matched control group of students not participating in the intervention and provides graphical and tabular information documenting the 
        comparability of the participant and control groups.</p>
        <br />
        <div class="sub-header"><h2>Data</h2></div><a id="sm1"></a>
        @Html.Raw(@Model.Report.DataText)
        <br />
        <div class="sub-header"><h2>Measured covariates</h2></div><a id="sm2"></a>
<p>The Evaluation Engine associates each program participant with 0-4
        students who were in the same grade when the program began,
        according to the SLDS, but were not identified as belonging to
        the participant group.  Although participants and their
        matched counterparts are required to have been enrolled at the
        same grade level when the intervention began, they are not
        required to be the same on any other specific pre-intervention
        characteristics.  Instead, the Evaluation Engine assembles
        information from the SLDS into variables describing
        participants and eligible counterparts at the outset of the
        intervention— the <em>measured
        covariates</em>— before selecting matches using
        <em>propensity scores</em><sup><a
        href="#footnote1">1</a></sup> based on combinations of
        measured covariates.  Characterized in terms of measured
        covariates, comparison groups assembled in this fashion
        ordinarily are near-equivalents of the participant groups they
        were matched to, although they may differ in terms of
        unmeasured baseline variables.  Accordingly, Evaluation Engine
        propensity scores use an expansive list of measured
        covariates<sup><a
        href="#footnote2">2</a></sup>.  </p>
<div class="pagebreak"></div>
        <div class="sub-header"><h2>Balance Plot</h2></div><a id="sm3"></a>
        <br />
        <!--<p style="padding-bottom:5px;color:#000;font-weight:600;">Figure A1: Balance Plot</p>-->
	<p style="padding-bottom:5px;font-weight:600;">Figure A1: Study group versus matched or statewide comparison group, at outset of the intervention</p>
        <br />
        <img src='@Url.Action("ChartImage", "Analysis", new { jobGUID = Model.Report.JobGUID, rank = 0, type = "balance" })' height = "850px" width = "850px" />
        <p style="padding-top:7px;"><span class="sm-font">Notes: The figure compares the study group to students statewide at the same grade level, and to the matched comparison group, in terms of averages of their baseline characteristics.  Horizontal positions of circular plotting symbols indicate the number of standard deviations by which study group means differed from state cohort means, whereas the x-axis positions of square plotting symbols indicate the number of standard deviations separating study group means from the matched counterpart means.</span></p>
	</div>
	<div class="indent">
	<br />
        <p>Measured covariates used in matching include familiar demographic and academic achievement variables, such 
        as sex, race and economic disadvantage, and performance on prior years’ achievement tests, are included, as are several baseline variables constructed for the Evaluation 
        Engine, such as “smoothed” versions of prior years’ test scores.<sup><a href="#footnote3">3</a></sup>  The main student-level variables used in the match are named in the left column of Appendix Figure A1. </p>
        <br />  
        <p>The graphic that appears to the right of the variable names in Figure A1 shows the magnitudes of difference between participants in the intervention and other
        students in the state, along with the extent to which the matching process mitigated these differences. Specifically, absolute differences between the means for the 
        intervention group and the means for all students in the same grade in the state are indicated with orange circles. The same differences for the participant group mean 
        and the matched comparison group mean are indicated with blue squares. Since the different variables are measured in different units, each variable is 
        standardized, that is, presented in multiples of the standard deviation of the variable in question. Thus, a variable having a symbol at 0.2 on the horizontal axis
        means that the participant group differed from the comparison group by 2/10 of a standard deviation.</p>
        <br />
        <p>When the intervention group's mean on a given variable was larger than that of the comparison group, the difference is represented with a solid (filled) circle or square; an unfilled symbol indicates that the comparison group's mean was larger. Figure A1 shows many but not all of the variables included in the matching process: besides a 
        few more student variables, for example dummy variables for whether the student had a score on previous years grade level tests, the Evaluation Engine also adjusts for 
        a number of school level variables, including school demographic and academic achievement profiles.</p>
        <br />
        <p>Figure A1 shows that after matching, differences between the two groups remain, even in terms of measured covariates. This is also true of randomized controlled 
        trials (RCTs), the “gold-standard” method of estimating program impacts. In finite samples, random assignment leaves small, random differences between treatment and 
        control groups. The Evaluation Engine aims to produce matched comparison groups that are about as similar to the participant group on measured characteristics as 
        typically occurs in paired RCTs of the same size.  This comparison to a hypothetical paired RCT can be made precise: in this analysis, if we could have randomly 
        reassigned participation in the intervention between actual participants and their matched counterparts, the ex ante probability of obtaining baseline differences 
        larger than those shown in Figure A1 would be @Model.Report.BalanceMainPval%<sup><a href="#footnote4">4</a></sup>.
	@if (!String.IsNullOrEmpty(Model.Report.BalanceMainPval) && Convert.ToDecimal(Model.Report.BalanceMainPval) < 5)
 {
	    @:(In this instance, unfortunately, even the matched differences shown in the figure are strictly larger than the chance imbalances of random assignment. The Evaluation Engine addresses this possibility with post-matching covariate adjustments, described below.)
	}
	else if (!String.IsNullOrEmpty(Model.Report.BalanceMainPval)) {
	    @:Alternately put, post-matching balance, in terms of the core covariates specifically mentioned in Figure A1, fell at the @Model.Report.BalanceMainPval% percentile (of the distribution of balance on these variables in comparably sized and configured RCTs).
	}
	When feasible, the matching procedure goes on to address imbalance at baseline on school- and student-level covariates in addition to those shown in Figure A1<sup><a href="#footnote5">5</a></sup>.</p>
        <br />
        <p>Propensity score matching aims to bring about a situation in which participant group means and matched comparison group means follow the same probability distributions, 
        for measured covariates and perhaps other variables<sup><a href="#footnote6">6</a></sup>, just as they would in an RCT.  Even when the match appears to have met its aims with regard to measured variables, 
        the impact estimates it generates carry two important limitations.</p>
        <br />
        <p>First and more important: Although the Evaluation Engine may approximate an RCT in terms of measured 
        covariates, it cannot replicate an RCT’s ability to balance unmeasured baseline variables. If at baseline participants differ systematically even from non-participants 
        who share their measured characteristics, and if these systematic differences in any way associate with outcomes, then the Evaluation Engine’s impact estimates will be 
        biased.  Such differences are typical of programs for which students, classrooms or schools were deliberately selected, and selected on the basis of characteristics 
        going beyond those recorded in the SLDS.<sup><a href="#footnote7">7</a></sup></p>
        <br />
        <p>Second, the matching procedure only emulates RCTs with <em>student-level</em> random assignment: for example, an RCT studying the 
        effectiveness of a program by following the winners and losers of a lottery for seats in the program. As compared to designs involving random assignment of schools or 
        classrooms, such RCTs may have difficulty documenting the effectiveness of programs which indirectly affect participants’ classmates or schoolmates, even those who are 
        not themselves program participants.<sup><a href="#footnote8">8</a></sup> Similarly, the Evaluation Engine’s approach is likely to lack power, by comparison with designs involving the matching of 
        classrooms or schools, for programs of this type.</p>
        <br />
        <div class="sub-header"><h2>Matching procedure</h2></div><a id="sm4"></a>
        <p>Matches are made only within grade levels (at the start of the intervention).  If the intervention group is particularly large, matches may also be restricted to fall within the levels of one or more of the subgroup analysis variables.  Matching within these groups involves three propensity scores.  </p>
        <br />
        <p>The first propensity score (PS) estimated models students’ participation in the intervention as a function of demographic and achievement characteristics of their 
        schools. Specifically, for each school the proportions of students falling in the various racial/ethnic, gender, economic disadvantage, English-language learner and 
        special-education categories, shown in Figure A1, are tabulated. Additionally, racial/ethnic diversity is represented as the probability that two students selected at 
        random from a given school would belong to the same racial/ethnic group.<sup><a href="#footnote9">9</a></sup> Finally, the school-level propensity modeling procedure incorporates empirical Bayes 
        estimates of school-level average achievement test results: in large schools, these averages fall very close to simple averages of student scores, whereas in smaller 
        schools they “borrow strength” from other, demographically similar schools by adjusting the school average score closer to the average of those similar schools. In the 
        school-level PS model, these school characteristics are used to characterize the likelihood of participation in @Model.Report.StudyName for this cohort.  No matches are 
        made on the basis of this first PS: rather, it is used to reduce the pool of controls eligible for matching to only those students attending schools not unlike schools 
        attended by program participants.</p>
        <br />
        <p>The second propensity score that is fit models participation as a function of student characteristics, the core measures shown in Figure A1. The first match that the 
        Evaluation Engine attempts is a match on the score from this model, termed the “core PS.”    This match is conducted to determine whether it is possible to balance the 
        core student level variables between participants and matched controls. If an acceptable level of balance, defined as the level that random assignment would have 
        achieved with probability .8,<!-- 1-BPVAL; see process.R--> is attained for core student measures, the process proceeds to attempt matching on other characteristics. In difficult matching problems, 
        however, the process stops after matching as closely as possible on the core propensity score only. In order to produce the best match, the matching routine permits 
        participants to share matched control group students, if doing so allows for closer matches on the core PS; at the same time, students in the participation group may be matched to up 
        to four controls.</p>
        <br />
        <p>If balance on core variables is found to be attainable, the algorithm attempts to match simultaneously on the core PS and on a third propensity score, the 
        “inclusive PS,” which predicts participation in the intervention based on student and school level variables concurrently. If by matching on a combination of the core 
        PS and the inclusive PS the algorithm is able to balance both core measures and the inclusive list of covariates, then it explores whether the match can be modified to 
        address several additional goals. First, it tries to match participation group students to comparable students in the same school districts.  This is a back-up measure 
        to ensure that matches come from similar school contexts<sup><a href="#footnote10">10</a></sup>, the primary measure being to match within the same state and to address school context variables in the 
        school and in the inclusive PS.  Second, it attempts to make matches within the same levels of a condensed disability categorization, to increase the likelihood that 
        program participants taking alternate forms of state tests will be matched to non-participants taking the same alternate test.<sup><a href="#footnote11">11</a></sup> Third, the procedure also aims to 
        select matches for participants from schools in which few or no other students participated in the same intervention, to avoid matching students in the participation 
        group to non-participants who may have been affected by the intervention indirectly, because of its presence at their schools -- (a “spillover” effect).<sup><a href="#footnote12">12</a></sup>  In this 
        analysis, @Model.Report.WithinDistrictMatchesPct% of matches were made within district.</p>
        <br />
        <p>Given the varied aims of the matching procedure, it is inevitable that situations will arise in which there are simply no non-participants available who are in all 
        respects good matches for a given participant.  The Evaluation Engine addresses this in several ways. First, prior to matching, participation group students whose 
        <em>inclusive</em> propensity scores differ measurably from those of each non-participant are removed from the analysis sample.<sup><a href="#footnote13">13</a></sup> (A side-effect of this is to impose overlap, 
        in terms of the inclusive PS, on the matched sample, by removing those participation group students whose propensity scores are very different from those of each 
        non-participant.)  Because the inclusive PS model coefficients have relatively large standard errors, participants are trimmed only if they are very different from the 
        non-participant group: this is a relatively permissive overlap requirement.</p>
        <br />
        <p>Second, the Evaluation Engine procedure sets priorities among the different propensity scores, devoting attention to the inclusive propensity score only after 
        confirming that matching on the main propensity score can closely balance the variables contributing to it, and devoting attention to within-district matching, within 
        disability category matching and to drawing matches from lower treatment density schools only after it has succeeded in balancing inclusive PS variables as well.  
        Matching well on all of these factors at once will not always be possible, but the Evaluation Engine ensures the best match, given what the data permits, by using 
        provably optimal matching algorithms.<sup><a href="#footnote14">14</a></sup></p>
        <br />
        <div class="sub-header"><h2>Outcome analysis</h2></div><a id="sm5"></a>
        <p>Significance tests begin by comparing treatment and comparison
	subjects on a given outcome.  The Evaluation Engine’s
	method of comparison uses a Winsorization procedure to limit
	the influence of outlying observations; for achievement test
	outcomes at grades 4-8, it also incorporates a robust
	covariance adjustment for the prior year's test scores.  These
	comparisons are then used in (normal-theory approximations to)
	permutation tests. Such tests assess the magnitude of an
	apparent trend in favor of the treatment group by repeatedly
	reassigning labels of students as treatment or control,
	independently in each matched set, and recording the apparent
	trend in favor of the notional treatment group that was
	created in this way; they then calculate the fraction of
	notional treatment groups enjoying advantages over the
	corresponding notional control groups as large or larger than
	the actual treatment group's advantage over its matched
	comparisons-the <em>p-value</em>.  When this p-value is small,
	chance alone cannot readily explain the apparent trend in
	favor of the treatment group. A second, complementary p-value
	calculation characterizes the plausibility of a chance
	explanation for an apparent trend to the advantage of the
	control group. If either of these “one-sided”
	p-values falls below 2.5%, the difference between the groups
	is deemed statistically significant.  When differences in scale 
        scores and in proficiency levels are presented together, the significance level shown is for the scale score difference, not the proficiency level difference.</p>
        <br /><br />
              <p>If subgroup analyses were requested, the significance levels that accompany them differ in meaning depending on whether the estimated overall effect was significant and in favor of the program. When participants' values of a given outcome are significantly higher than those of their matched counterparts, the subgroup analysis that follows tests whether the benefit experienced by the selected subgroups differed from the overall average benefit of participation in the intervention program. On the other hand, if participants' values tend to be lower than those of their matched counterparts (or if there is no consistent and statistically meaningful trend) then the Evaluation Engine tests for program benefits separately within each subgroup.  The results of the two different types of subgroup analyses are presented in a similarly structured table.</p>        <br />
  <div class="sub-header"><h2>Statistically significant main effect</h2></div><a id="sm6"></a>
<p>In the case of a statistically significant main effect, covariances of main and subgroup effect estimates are estimated by an adaption of the method of the paired t test.<sup><a href="#footnote16">16</a></sup>  Because there can be many such tests, based on the number of categories in the subgroups, the chance of one or more spurious significant findings is high. To address this possibility, the Evaluation Engine incorporates a correction based the method of Hothorn <em>et al</em>.<sup><a href="#footnote15">15</a></sup></p> 
        <br />
<div class="sub-header"><h2>Non-significant main effect </h2></div><a id="sm7"></a>
        <p>In the event that this overall test is not statistically significant, then any subsequent subgroup analysis (if subgroup analysis was requested) is a separate permutation 
        t-test for the presence of an intervention effect within each of the designated subgroups. Since the number of tests is large, the method of Hothorn <em>et al</em>. 
        is used for multiplicity corrections.</p>
        <br />
        <div class="sub-header"><h2>Effect estimates</h2></div><a id="sm8"></a>
        <p>Estimates of the intervention effect are calculated with “effect of treatment on the treated” weighting.  Specifically, for each student in the participant group matched to 
        one or more non-participating students, the effect of the intervention is estimated as the difference between the participant’s outcome and the average outcome among the 
        participant's matched counterparts. The second of these numbers is called the participant’s <em>matched counterpart value</em>.  In simple cases, overall intervention impacts are 
        estimated as averages of estimated effects on program participants—that is, as means of differences between participants' outcomes and corresponding  
        matched counterpart values.</p>
	<br />
	<p>The calculation is more complex if there are participants who could not be matched, if outcome information was unavailable for one or more matched subjects or if some participants differed from their matched counterparts in terms of key demographic characteristics.  If there are participants who could not be matched, they do not contribute to the estimate of the program's benefit. (The estimate is of the effect of treatment on <em>those treated subjects who could be matched</em>, not all treated subjects irrespective of matchability.)  Participants matched only to controls lacking a value of a given variable do not contribute to the variable's participant mean, nor do these controls contribute to the variable's matched counterpart mean. Similarly, in order to contribute to the matched counterpart mean of a given variable, a non-participating student must be matched to a participant who is not missing a value on that variable.   (Balance calculations apply similar rules when missing values are found in baseline variables.)</p>
	<br />
	<p>If some participants were matched to counterparts differing from them on sex, race/ethnicity, economic disadvantage, special education (IDEA), or limited English proficiency, participant minus matched comparison value differences are calculated on indicator variables encoding each demographic categorization.  Then a linear model is used to adjust outcome differences, participants' outcomes minus corresponding matched comparison values, for these demographic covariate differences. (In contrast with models used for significance testing, this model is fit by ordinary rather than robust regression; it does not attempt to adjust for prior test scores.) The fitted model's intercept term, a covariance adjusted mean difference of participant and matched comparison outcomes, serves as the estimate of the program's benefit.</p>

        <br />
<!-- NB: `eem` references anchor below. If id changes, update `eem` constant link.to.sensitivity.section.of.Report-->
<a id="sec8"></a>
        <div class="sub-header"><h2>Sensitivity to unmeasured variables</h2></div><a id="sm9"></a>
        <p>As a quasi-experimental method, the Evaluation Engine propensity match cannot ensure comparability of the groups in terms of unmeasured variables.  Propensity score matching 
        may help correct for some unmeasured variables, particularly those that are highly correlated with measured variables included in the matching; but unmeasured between-group 
        differences can, if they are highly predictive of outcomes, greatly bias the Evaluation Engine's impact estimates.  Ultimately it falls to the researcher to assess the potential 
        for biases of this kind. Appendix Figure A2 offers some assistance in making these assessments.</p>
        <br />
        <p>If one or more outcome difference was found to be statistically significant, the extent of that finding's omitted variable sensitivity<sup><a id="fnsource17" href="#footnote17">17</a></sup> is recorded beneath the x-axis of Figure A2.  The markings that appear there indicate the minimum extent of the difference between students in the participation and 
        matched control groups, in standard deviations of the hypothetical unmeasured variable, that would be necessary for that variable to explain away the impact finding (in the sense of rendering it statistically insignificant). Such 
        an unmeasured variable could more readily upset a finding if it strongly predicted the relevant outcome than if its association with that outcome were weak.  To represent the 
        range of possibilities, therefore, for each statistically significant impact estimate the figure indicates two estimates of the extent of unmeasured difference necessary for 
        spurious statistical significance: an uppercase letter, “R” for a reading achievement outcome, “M” for math and so forth, indicates the requisite imbalance for a hypothetical 
        omitted variable that predicts the outcome strongly, having a partial correlation with that outcome of 0.75; whereas the horizontal position of the corresponding lowercase 
        letter indicates the extent of confounding necessary for a weaker predictor of that outcome, one having a partial correlation with it of 0.1, to cause spurious statistical 
        significance.</p>
        <br />
        <p>Partial correlations of .75 are quite high: prior achievement tests in the same subject as the posttest typically have partial correlations to the posttest of this 
        magnitude; but few other baseline/outcome pairs of variables relate this strongly.  Demographic variables' correlations with achievement outcomes are commonly about .1.<sup><a href="#footnote18">18</a></sup></p>
        <br />  
        <p>To analyze the sensitivity of a significant finding to a specific omitted variable, situate the omitted variable between these extremes in terms of its likely relationship 
        with the outcome, and then consider what degree of participant-matched control difference on the variable is plausible.  In some cases, hypotheses about this difference are 
        easier to formulate and understand by comparing them to measured variables.  In such a case, one can calibrate sensitivity by comparing measured differences from the upper 
        part of the plot to sensitivity thresholds on the bottom of the plot.  For example, consider an unmeasured variable with a partial correlation to the grade level reading test 
        outcome, say, of 0.1, with differences between the participant group and matched controls on par with these groups' pre-matching difference in eligibility for free or reduced-price lunch, for 
        instance.   Such an omitted variable could sufficiently bias an estimated grade level reading effect to make it falsely statistically significant only if the red circle for Free/reduced price lunch falls to the right of the small “r” symbol at the bottom of the plot. An omitted variable on which participants and matched controls are imbalanced to a 
        degree similar to this, but which correlates more strongly with the reading outcome, could more readily produce spurious statistical significance.  Specifically, for an 
        unmeasured variable with partial correlation to the same reading outcome of .75, the appropriate reference point is the uppercase “R” at the bottom of the figure. </p>
	<br/>
	<p style="padding-bottom:5px;font-weight:600;">Figure A2: Sensitivity of outcome findings to omitted variable confounding (below x-axis) against measured baseline differences (above x-axis)</p>
        <br />
        <a href="" onclick="window.open('@Url.Action("ChartImage", "Analysis", new { jobGUID = Model.Report.JobGUID, rank = 0, type = "balance-2" })', 'newwindow', 'scrollbars=1, width=850, height=1200'); return false;">[Click here to open in new window]</a>
        <img src='@Url.Action("ChartImage", "Analysis", new { jobGUID = Model.Report.JobGUID, rank = 0, type = "balance-2" })' height = "650px" width = "650px" />
	<p style="padding-top: 7px;"><span class="sm-font">Notes: The figure compares the study group to students statewide at the same grade level, and to the matched comparison group selected by the Evaluation Engine.  Differences are expressed as a proportion of each variable's standard deviation. When estimated program impacts reach statistical significance, the magnitude of the difference in a hypothetical omitted variable that would be needed to upset the finding is indicated beneath the x axis.</span></p>
	<br/>
        <hr />
	<!-- NB: END Data and Statistical Methods SECTION TEXT-->
        <div class="footnotes">
        <p><a id="footnote1"></a><sup>1</sup> P.R. Rosenbaum and D.B. Rubin, “The central role of the propensity score in observational studies for causal effects,” <em>Biometrika</em>, 70:41–55,  1983.</p><br />
        <p><a id="footnote2"></a><sup>2</sup> Indeed, an important reason the Evaluation Engine uses propensity score matching is the capability of the technique to address more baseline variables than other common methods of adjustment for 
        quasi-experiments.  See [Rubin and Thomas, 1996].</p><br />
        <p><a id="footnote3"></a><sup>3</sup> This smoothing involves the use of so-called “Empirical Bayes” techniques to average students’ observed test scores with mean scores at their schools and demographic groups. The smoothed scores 
        permit meaningful substitute scores for students missing a baseline test result, and they limit bias from test measurement error in models using test scores as an explanatory variable.  Because the 
        smoothed scores are calculated using baseline information exclusively, they are pre-intervention variables, and matching on them does not contaminate the outcome comparison (D.B. Rubin, “For objective 
        causal inference, design trumps analysis,” <em>Annals of Applied Statistics</em>, 2(3):808–40, 2008).</p><br />
        <p><a id="footnote4"></a><sup>4</sup> The Evaluation Engine’s covariate balance summary follows Hansen, B.B. and Bowers, J., “Covariate balance in simple, stratified and clustered comparative studies,” <em>Statistical Science</em>, 
        23(2):219–236,  2008.</p><br />
        <p><a id="footnote5"></a><sup>5</sup>For this evaluation of @Model.Report.AnalysisName, post-matching balance on the full list of 
        covariates, core covariates plus school-level variables and additional student measures, was at the @Model.Report.BalanceInclusivePval percentile of balance (on comparable
            collections of covariates and in comparable RCTs). See the <a href="#SupplementalInfo">Supplemental Statistical Material</a> for additional detail.</p><br />
        <p><a id="footnote6"></a><sup>6</sup> See Rosenbaum, P.R. and Rubin, D.B, “The central role of the propensity score in observational studies for causal effects.” <em>Biometrika</em>, 70:41–55, 1983; Rubin, D.B., “Practical implications of modes 
        of statistical inference for causal effects and the  critical role of the assignment mechanism,” <em>Biometrics</em>, 47:1213–1234, 1991; or Braitman, L.E. and Rosenbaum, P.R., “Rare Outcomes, Common 
        Treatments: Analytic  Strategies Using Propensity Scores,” <em>Annals of Internal Medicine</em>,  137(8):693–695, 2002.</p><br />
        <p><a id="footnote7"></a><sup>7</sup> Specific selection scenarios can sometimes be translated into plausible ranges of bias, and in turn into how much statistically adjusting for an unmeasured variable would have caused study findings 
        to change, with the help of the lettered annotations in the bottom margin of Figure A1.  See the discussion “Sensitivity to Unmeasured Variables,” above.</p><br />
        <p><a id="footnote8"></a><sup>8</sup> See e.g. Raudenbush, S.W., “Statistical analysis and optimal design for cluster randomized trials,” <em>Psychological Methods</em>, 2:173–185, 1997; Sobel, M.E., “What do randomized studies of housing 
        mobility demonstrate?: Causal  inference in the face of interference,” <em>Journal of the American  Statistical Association</em>, 101(476):1398–1407, 2006.</p><br />
        <p><a id="footnote9"></a><sup>9</sup> In other words, one of the independent variables contributing to the school level PS is the probability of interspecific encounter (Hurlbert, S.H., “The nonconcept of species diversity: a 
        critique and alternative parameters” <em>Ecology</em>,  52(4):577–586, 1971) as applied to race/ethnicity.  Lucas and Berends (“Sociodemographic  diversity, correlated achievement, and de facto tracking,” 
        <em>Sociology of  Education</em>, pages 328–348, 2002) present a similar measure, linking it to racially disparate tracking.</p><br />
        <p><a id="footnote10"></a><sup>10</sup> Cook, Shadish and Wong’s review (“Three conditions under which experiments and observational studies produce comparable causal estimates:  New findings from within-study comparisons,” 
        <em>Journal of Policy Analysis and Management</em>, 27(4):724–750, 2008) identified matching within local contexts as one attribute of quasi-experiments appearing to have successfully addressed selection bias.  Bifulco’s (“Can 
        nonexperimental estimates replicate estimates based on random assignment in  evaluations of school choice? A within-study comparison” <em>Journal of Policy Analysis and Management</em>, 31(3):729–751, 2012) case 
        study suggests that propensity matches from similar districts elsewhere within a state may be as good or better than same-district matches.</p><br />
        <p><a id="footnote11"></a><sup>11</sup> In order that the variables used in matching precede the start of the program, at the time of matching the Evaluation Engine does not take into account whether a student’s outcome test scores come 
        from the standard or from alternate forms of the test.  Instead, it attempts to avoid matching across boundaries of the following categories: not disabled; deaf and blind, and took alternate assessment 
        in previous year; deaf and blind, but did not take alternate assessment in previous year (including no score recorded); autistic, and took alternate assessment in previous year; autistic, and did not 
        take alternate assessment in previous year; other disabled, with likely mild or no cognitive impairment (“specific learning disability”, “speech or language impairment”,…); other disabled, with likely 
        cognitive impairment (“traumatic brain injury”, “intellectual disability”, “multiple disability”). </p><br />
        <p><a id="footnote12"></a><sup>12</sup> The possibility of spillover effects is also addressed by the use of permutation methods for the significance tests reported with our overall average effect estimates; these remain statistically 
        valid, in terms of their basic interpretations, in the presence of spillover, although spillover can limit their power.</p><br />
        <p><a id="footnote13"></a><sup>13</sup> More formally, participation group students whose inclusive propensity score differs from each non-participant of similar disability status, to an extent that estimation error in the propensity score 
        could not readily explain. This trimming of the sample is a by-product of broader requirements that matching be done within calipers of the inclusive PS (e.g., Rubin, D.B. and Thomas, N., “Matching using estimated 
        propensity scores: Relating theory to  practice,” <em>Biometrics</em>, 52:249–64, 1996), the caliper width being a function of the magnitude of estimation error, and that matched counterparts 
        not differ significantly from one another in terms of their estimated inclusive propensity scores, in multiplicity-corrected significance tests. Because the inclusive PS model coefficients have 
        relatively large standard errors, participants are trimmed only if they are very different from the non-participant group.</p><br />
        <p><a id="footnote14"></a><sup>14</sup> Gu, X.S. and Rosenbaum, P.R., “Comparison of multivariate matching methods: Structures, distances, and algorithms,” <em>Journal of Computational and Graphical  Statistics</em>, 2(4):405–420, 1993; Hansen, 
        B.B. and Klopfer, S.O., “Optimal full matching and related  designs via network flows,” Journal of Computational and Graphical  Statistics, 15(3):609–627, 2006.</p><br />
        <p><a id="footnote15"></a><sup>15</sup> Hothorn T., Bretz F., and Westfall P., “Simultaneous inference in general parametric models,” <em>Biometrical  Journal</em>, 50(3):346–363, 2008.</p><br />
        <p><a id="footnote16"></a><sup>16</sup> Specifically, the method of Abadie and Imbens (“A martingale representation for matching estimators,” <em>Journal of the American Statistical Association</em>, 107(498):833–843, 2012), adapted to full 
        matching (Rosenbaum, P.R., “A  characterization of optimal designs for observational studies,” <em>Journal of  the Royal Statistical Society</em>, 53:597–610, 1991) and to covariance as well as variance 
        estimation.</p><br />
        <p><a id="footnote17"></a><sup>17</sup> In the tradition of Cornfield et al. (“Smoking and lung cancer: Recent evidence and a discussion of some questions,” <em>Journal of the National Cancer Institute</em>, 22:173–203, 1959).  The method adapts 
        that of Hosman <em>et al</em>. (“The sensitivity of linear regression  coefficients' confidence limits to the omission of a confounder,” Annals  of Applied Statistics, 4(2):849–870, 2010) to the Evaluation 
        Engine’s estimation strategies, and to present sensitivity parameters in units comparable to those of Figure A1.</p><br />
        <p><a id="footnote18"></a><sup>18</sup> The partial correlations in question can be recovered from a quantity of secondary interest in the planning of group randomized trials, Hedges and Hedberg’s η<sub>W</sub> (“Intraclass correlation values for 
        planning group-randomized trials in education,” <em>Educational Evaluation and Policy Analysis</em>, 29(1):60–87, 2007) .  Hedges and Hedberg tabulate values of this quantity from nationally representative 
        samples. While their results varied somewhat across grade and subject, the implied partial correlations generally fell in the neighborhoods of .75 (partial correlations with pretests) and .01 
        (demographic variables).</p>
	    <p>The Evaluation Engine does match on both demographic and prior achievement variables.  Interestingly, however, the same reference values can be motivated by a study of effects of baseline variables 
        that the Evaluation Engine does not address, namely classroom or teacher achievement data.  Zhu, Jacob, Bloom and Xu (“Designing and analyzing studies that randomize schools to estimate intervention 
        effects on student academic outcomes without classroom-level information,” <em>Educational Evaluation and Policy Analysis</em>, 34(1):45–68, 2012) quantitatively characterize the additional value of 
        classroom-level data over and above individual- and school-level data, for the planning and analysis of randomized trials. In the lower grades, classroom-level data turns out to add relatively little 
        from the trialist's perspective, while in high school it makes a meaningful contribution. Back-translating from their measures to ours, results again vary somewhat across grades, tests and data sources, 
        but partial correlations of .1 typify the classroom data’s additional contribution in the lower grades while .75 typifies their contribution to prediction of high school achievement outcomes.</p><br />
        </div>
        <div class="pagebreak"></div>
		<div class="section-header"><h1>4. User Descriptions</h1></div><a id="UserDescriptions" ></a>
		<br />
        <div class="sub-header"><span>Name of intervention group</span></div>
		<p>@Model.Report.StudyName</p>
        <br />
        <div class="sub-header"><span>Description of intervention</span></div>
        <p>@Model.Report.StudyDescription</p>
        <br />
        <div class="sub-header"><span>Title of report</span></div>
		<p>@Model.Report.AnalysisName</p>
        <br />
        <div class="sub-header"><span>Description of analysis</span></div>
	    <p>@Model.Report.AnalysisDescription</p>
	    <p><p>@Html.Raw(Model.Report.LibraryEntry.Params)</p></p>
	    <div class="pagebreak"></div>
	    <div class="supplemental"><a id="SupplementalInfo" ></a>
        @Html.Raw(@Model.Report.SupplementalInformation)
        </div>
      </div>
</body>
</html>